# ====== kubermatic ======
kubermatic:
  # Whether the cluster is a master cluster
  # This must be false for any non-master seed
  isMaster: true
  # the base64 encoded docker/quay authentication json file
  imagePullSecretData: "TODO ADD PULL SECRET BASE64"
  auth:
    # the full path to the openid connect token issuer. For example 'https://dev.kubermatic.io/dex'
    tokenIssuer: "https://kubermatic.TODO-STUDENT-DNS.loodse.training/dex"
    # the client id for openid connect
    clientID: "kubermatic"
    issuerRedirectURL: "https://kubermatic.TODO-STUDENT-DNS.loodse.training/api/v1/kubeconfig"
    issuerClientID: "kubermaticIssuer"
    issuerClientSecret: "6QPSd2eCjzcMWEnYbM6f"
    #random: created by `openssl rand -hex 32`
    issuerCookieKey: "cbfb027a5cb465143c949b7b1f4935b624a64a778984cd0c250e5d56f6883b0f"
    # the service account signing key. Must be 32 bytes or longer
    serviceAccountKey: "5333c1993972b48791312a23c6985547c47b0d4c819bd8debf459ec3e2a5759c"
  # base64 encoded datacenters.yaml
  datacenters: "TODO ADD DATACENTER BASE64"
  # external domain for the kubermatic installation. For example 'dev.kubermatic.io'
  domain: "kubermatic.TODO-STUDENT-DNS.loodse.training"
  # base64 encoded kubeconfig which gives admin access to all seed clusters
  kubeconfig: "TODO KUBECONFIG BASE64 ENCODED"

  controller:
    datacenterName: k1

  # Whether to load the datacenters from CRDs dynamically during runtime
  dynamicDatacenters: false

  # helm hooks/checks
  checks:
    # Checks if the last release contains the kubermatic CRD's.
    # We moved them out of the chart to avoid issues with helm
    crd:
      disable: false
  projects_migrator:
    dry_run: false
  vpa:
    recommender:
      resources:
        limits:
          memory: 4Gi
      tolerations:
      - effect: NoSchedule
        operator: Exists
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: kubermatic.io/type
                operator: In
                values:
                - stable

### Nginx definition
nginx:
  hostNetwork: false
  asDaemonSet: false
  replicas: 3

certificates:
  domains:
  - "kubermatic.TODO-STUDENT-DNS.loodse.training"
  - "alertmanager.kubermatic.TODO-STUDENT-DNS.loodse.training"
  - "grafana.kubermatic.TODO-STUDENT-DNS.loodse.training"
  - "prometheus.kubermatic.TODO-STUDENT-DNS.loodse.training"
  - "kibana.kubermatic.TODO-STUDENT-DNS.loodse.training"

### Monitoring
prometheus:
  host: "prometheus.kubermatic.TODO-STUDENT-DNS.loodse.training"
  externalLabels:
    seed_cluster: k1
  ruleFiles:
  - /etc/prometheus/rules/general-*.yaml
  - /etc/prometheus/rules/kubermatic-*.yaml
  tolerations:
  - effect: NoSchedule
    operator: Exists
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: kubermatic.io/type
            operator: In
            values:
            - stable
  scraping:
    configs:
    - job_name: 'blackbox'
      metrics_path: /probe
      params:
        module: [https_2xx]
      static_configs:
      - targets:
        - https://kubermatic.TODO-STUDENT-DNS.loodse.training/
        - https://prometheus.kubermatic.TODO-STUDENT-DNS.loodse.training/-/healthy
        - https://alertmanager.kubermatic.TODO-STUDENT-DNS.loodse.training/-/healthy
        - https://grafana.kubermatic.TODO-STUDENT-DNS.loodse.training/api/health
        - https://kibana.kubermatic.TODO-STUDENT-DNS.loodse.training/ui/favicons/favicon.ico
      relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - source_labels: [__param_module]
        target_label: module
      - source_labels: [__address__]
        action: replace
        replacement: blackbox-exporter:9115
        target_label: __address__
  # becomes relevant when migrating to Kubermatic 2.11
  migration:
    enabled: false

grafana:
  user: YWRtaW4= # admin
  password: T2dLcXFRYlZIYXdS # OgKqqQbVHawR
  host: "grafana.kubermatic.TODO-STUDENT-DNS.loodse.training"
  provisioning:
    configuration:
      auto_assign_org_role: Editor
      disable_login_form: false
  dashboards:
    editable: true

alertmanager:
  host: "alertmanager.kubermatic.TODO-STUDENT-DNS.loodse.training"
  config:
    global:
      slack_api_url: https://hooks.slack.com/services/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
    route:
      receiver: 'default-receiver'
      group_by: ['alertname', 'cluster', 'seed_cluster']
      group_wait: 10s
      group_interval: 5m
      repeat_interval: 1h
    receivers:
    - name: 'blackhole'
    - name: 'default-receiver'
      slack_configs:
      - channel: '#alerting-run'
        send_resolved: true
        icon_emoji: ':prometheus:'
        title: '{{ template "slack.kubermatic.title" . }}'
        text: '{{ template "slack.kubermatic.text" . }}'
    - name: 'notify'
      slack_configs:
      - channel: '#alerting-run'
        send_resolved: true
        icon_emoji: ':prometheus:'
        title: '{{ template "slack.kubermatic.title" . }}'
        text: '{{ template "slack.kubermatic.text" . }}'
    templates:
    - '*.tmpl'
  # becomes relevant when migrating to Kubermatic 2.11
  migration:
    enabled: false

dex:
  ingress:
    host: "kubermatic.TODO-STUDENT-DNS.loodse.training"
  clients:
  - id: kubermatic
    name: Kubermatic
    secret: ZXhhbXBsZS1hcHAtc2VjcmV0
    RedirectURIs:
    - http://localhost:8000
    - https://kubermatic.TODO-STUDENT-DNS.loodse.training
    - http://localhost:8000/clusters
    - https://kubermatic.TODO-STUDENT-DNS.loodse.training/clusters
    - http://localhost:8000/projects
    - https://kubermatic.TODO-STUDENT-DNS.loodse.training/projects
    #OIDC account
  - id: kubermaticIssuer
    name: KubermaticIssuer
    secret: 6QPSd2eCjzcMWEnYbM6f
    RedirectURIs:
    - http://localhost:8080/api/v1/kubeconfig
    - https://kubermatic.TODO-STUDENT-DNS.loodse.training/api/v1/kubeconfig
  - id: grafana
    name: grafana
    secret: CVq2JKiR3Trpbv5I2Jqx
    RedirectURIs:
    - https://grafana.kubermatic.TODO-STUDENT-DNS.loodse.training/oauth/callback
  - id: kibana
    name: kibana
    secret: lATJ0epyUg9RrLmZeLNp
    RedirectURIs:
    - https://kibana.kubermatic.TODO-STUDENT-DNS.loodse.training/oauth/callback
  - id: prometheus
    name: prometheus
    secret: xbAIBXHMzl3PKUvdaeln
    RedirectURIs:
    - https://prometheus.kubermatic.TODO-STUDENT-DNS.loodse.training/oauth/callback
  - id: alertmanager
    name: alertmanager
    secret: hIiDEZnlcHvo9MinTHfU
    RedirectURIs:
    - https://alertmanager.kubermatic.TODO-STUDENT-DNS.loodse.training/oauth/callback
  connectors:
  # see https://github.com/dexidp/dex/blob/master/Documentation/connectors/oidc.md
  - type: oidc
    id: google
    name: Google
    config:
      # oauth client creation: https://console.cloud.google.com/apis/credentials
      issuer: https://accounts.google.com
      clientID: TODO-GOOGLE-CLIENT-ID
      clientSecret: TODO-GOOGLE-CLIENT-SECRET
      redirectURI: https://kubermatic.TODO-STUDENT-DNS.loodse.training/dex/callback
#  - type: github
#    id: github
#    name: GitHub
#    config:
#      clientID: xxxxxxxxxx
#      clientSecret: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#      redirectURI: https://kubermatic.TODO-STUDENT-DNS.loodse.training/dex/callback
#      orgs:
#      - name: STUDENT-GIT-ORG

iap:
  deployments:
    grafana:
      name: grafana
      client_id: grafana
      client_secret: CVq2JKiR3Trpbv5I2Jqx
      encryption_key: QZCI1QPxZozKvanPquxv
      upstream_service: grafana.monitoring.svc.cluster.local
      upstream_port: 3000
      ingress:
        host: "grafana.kubermatic.TODO-STUDENT-DNS.loodse.training"
      config:
        enable-authorization-header: false
      passthrough:
      - /api/health
    kibana:
      name: kibana
      client_id: kibana
      client_secret: lATJ0epyUg9RrLmZeLNp
      encryption_key: aL5TeZFXwp6D5fLjNkU4
      upstream_service: kibana-logging.logging.svc.cluster.local
      upstream_port: 5601
      ingress:
        host: "kibana.kubermatic.TODO-STUDENT-DNS.loodse.training"
      passthrough:
      - /ui/favicons/favicon.ico
    prometheus:
      name: prometheus
      client_id: prometheus
      client_secret: xbAIBXHMzl3PKUvdaeln
      encryption_key: gVgaYM5VvXF1vEnQJmQJ
      # update this when upgrading to Kubermatic 2.11
      upstream_service: prometheus.monitoring.svc.cluster.local
      upstream_port: 9090
      ingress:
        host: "prometheus.kubermatic.TODO-STUDENT-DNS.loodse.training"
      passthrough:
      - /-/healthy
    alertmanager:
      name: alertmanager
      client_id: alertmanager
      client_secret: hIiDEZnlcHvo9MinTHfU
      encryption_key: G1lSG6eH914HHQhpKWMU
      # update this when upgrading to Kubermatic 2.11
      upstream_service: alertmanager.monitoring.svc.cluster.local
      upstream_port: 9093
      ingress:
        host: "alertmanager.kubermatic.TODO-STUDENT-DNS.loodse.training"
      passthrough:
      - /-/healthy
  discovery_url: https://kubermatic.TODO-STUDENT-DNS.loodse.training/dex/.well-known/openid-configuration
  port: 3000

minio:
  storeSize: "100Gi"
  credentials:
    accessKey: "reoshe9Eiwei2ku5foB6owiva2Sheeth"
    secretKey: "rooNgohsh4ohJo7aefoofeiTae4poht0cohxua5eithiexu7quieng5ailoosha8"

logging:
  elasticsearch:
    dataReplicas: 3
    storageSize: 100Gi